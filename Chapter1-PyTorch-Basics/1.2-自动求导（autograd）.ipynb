{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单情况的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([2]), requires_grad=True)\n",
    "y = x + 2\n",
    "z = y ** 2 + 3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "# 求对x的导数\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复杂情况的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6134, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(10, 20), requires_grad=True)\n",
    "y = Variable(torch.randn(10, 5), requires_grad=True)\n",
    "w = Variable(torch.randn(20, 5), requires_grad=True)\n",
    "\n",
    "out = torch.mean(y - torch.matmul(x, w))\n",
    "print(out)\n",
    "# torch.mean()：求整个tensor的均值\n",
    "# torch.matmul(A, B)：矩阵乘法：A x B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088],\n",
      "        [-0.0031, -0.0025,  0.0180, -0.0421,  0.0775,  0.0173,  0.0959, -0.0819,\n",
      "         -0.0808,  0.0176, -0.0060,  0.0045,  0.0164,  0.0288,  0.0493, -0.0328,\n",
      "          0.0390,  0.0009, -0.0534,  0.0088]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200]])\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0601, -0.0601, -0.0601, -0.0601, -0.0601],\n",
      "        [-0.0401, -0.0401, -0.0401, -0.0401, -0.0401],\n",
      "        [-0.0942, -0.0942, -0.0942, -0.0942, -0.0942],\n",
      "        [-0.0255, -0.0255, -0.0255, -0.0255, -0.0255],\n",
      "        [-0.0024, -0.0024, -0.0024, -0.0024, -0.0024],\n",
      "        [ 0.0261,  0.0261,  0.0261,  0.0261,  0.0261],\n",
      "        [ 0.0312,  0.0312,  0.0312,  0.0312,  0.0312],\n",
      "        [-0.0440, -0.0440, -0.0440, -0.0440, -0.0440],\n",
      "        [ 0.0272,  0.0272,  0.0272,  0.0272,  0.0272],\n",
      "        [ 0.0009,  0.0009,  0.0009,  0.0009,  0.0009],\n",
      "        [ 0.0217,  0.0217,  0.0217,  0.0217,  0.0217],\n",
      "        [ 0.0332,  0.0332,  0.0332,  0.0332,  0.0332],\n",
      "        [ 0.0184,  0.0184,  0.0184,  0.0184,  0.0184],\n",
      "        [-0.0171, -0.0171, -0.0171, -0.0171, -0.0171],\n",
      "        [-0.1415, -0.1415, -0.1415, -0.1415, -0.1415],\n",
      "        [-0.0035, -0.0035, -0.0035, -0.0035, -0.0035],\n",
      "        [-0.0308, -0.0308, -0.0308, -0.0308, -0.0308],\n",
      "        [-0.0667, -0.0667, -0.0667, -0.0667, -0.0667],\n",
      "        [ 0.0301,  0.0301,  0.0301,  0.0301,  0.0301],\n",
      "        [-0.0640, -0.0640, -0.0640, -0.0640, -0.0640]])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.]], requires_grad=True)\n",
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "m = Variable(torch.FloatTensor([[2, 3]]), requires_grad=True)\n",
    "n = Variable(torch.zeros(1, 2))\n",
    "\n",
    "print(m)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 27.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "n[0, 0] = m[0, 0] ** 2\n",
    "n[0, 1] = m[0, 1] ** 3\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将上面写成公式为：$n=(n_0, n_1)=(m_0^2, m_1^3)$\n",
    "### 那么n对m的导数为：$\\frac{\\partial n}{\\partial m}=(\\frac{\\partial n}{\\partial m_0},\\frac{\\partial n}{\\partial m_1})$\n",
    "## 在 PyTorch 中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和 n 一样大，比如是$(w_0,w_1)$\n",
    "## 其中，$\\frac{\\partial n}{\\partial m_0}=w_0\\frac{\\partial n_0}{\\partial m_0}+ w_1\\frac{\\partial n_1}{\\partial m_0}$\n",
    "## $\\frac{\\partial n}{\\partial m_1}=w_0\\frac{\\partial n_0}{\\partial m_1}+ w_1\\frac{\\partial n_1}{\\partial m_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 27.]])\n"
     ]
    }
   ],
   "source": [
    "n.backward(torch.ones_like(n))  # 将(w0, w1)规定为(1, 1)\n",
    "print(m.grad)\n",
    "# torch.ones_like(n)：生成一个大小和n一样的、值全为1的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多次自动求导\n",
    "### 通过调用 backward 我们可以进行一次自动求导，如果我们再调用一次 backward，会发现程序报错，没有办法再做一次。这是因为 PyTorch 默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西，我们通过下面的小例子来说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([3]), requires_grad=True)\n",
    "y = x * 2 + x ** 2 + 3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)  # 通过设置retain_graph为True来保留计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
