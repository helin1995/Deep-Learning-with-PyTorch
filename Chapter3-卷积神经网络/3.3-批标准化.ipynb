{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批标准化\n",
    "#### 在我们正式进入模型的构建和训练之前，我们会先讲一讲数据预处理和批标准化，因为模型训练并不容易，特别是一些非常复杂的模型，并不能非常好的训练得到收敛的结果，所以对数据增加一些预处理，同时使用批标准化能够得到非常好的收敛结果，这也是卷积网络能够训练到非常深的层的一个重要原因。\n",
    "### 1、数据预处理\n",
    "#### 目前数据预处理最常见的方法就是中心化和标准化，中心化相当于修正数据的中心位置，实现方法非常简单，就是在每个特征维度上减去对应的均值，最后得到 0 均值的特征。标准化也非常简单，在数据变成 0 均值之后，为了使得不同的特征维度有着相同的规模，可以除以标准差近似为一个标准正态分布，也可以依据最大值和最小值将其转化为 -1 ~ 1 之间。\n",
    "### 2、批标准化\n",
    "#### 前面在数据预处理的时候，我们尽量输入特征不相关且满足一个标准的正态分布，这样模型的表现一般也较好。但是对于很深的网路结构，网路的非线性层会使得输出的结果变得相关，且不再满足一个标准的 N(0, 1) 的分布，甚至输出的中心已经发生了偏移，这对于模型的训练，特别是深层的模型训练非常的困难。所以在 2015 年一篇论文提出了这个方法，批标准化，简而言之，就是对于每一层网络的输出，对其做一个归一化，使其服从标准的正态分布，这样后一层网络的输入也是一个标准的正态分布，所以能够比较好的进行训练，加快收敛速度。\n",
    "#### 在这里暂时不详细阐述批标准化的实现，我们只需要知道在PyTorch中如何使用批标准化。我们就以3.2节的代码为基础，在其中加入批标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import mnist  # 导入PyTorch中内置的mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化，直接采用参考文档代码\n",
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5      # 标准化，这个技巧之后会讲到\n",
    "    # x = x.reshape((-1,))   # 拉平\n",
    "    x = torch.from_numpy(x)  # 将numpy数组转换为tensor\n",
    "    x = x.unsqueeze(0)       # 增加channel，设置为1。图片的大小为：(1, 28, 28)\n",
    "    return x\n",
    "# 重新加载数据集，申明定义的数据变换\n",
    "train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True)\n",
    "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在pytorch中，实现批标准化是使用nn.BatchNorm2d(output_depth)，其中output_depth是这一层输出的图片深度。\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 第一层：数据输入是：(batch, 1, 28, 28)，经过卷积，第一层的输出则是：(batch, 25, 26, 26)\n",
    "        self.layer1 = nn.Sequential(\n",
    "                      nn.Conv2d(1, 25, 3),\n",
    "                      nn.BatchNorm2d(25),  # 25就是输出的深度\n",
    "                      nn.ReLU())\n",
    "        # 第二层是池化层，池化窗口大小为(2, 2)，数据输入是：(batch, 25, 26, 26)，经过池化，第二层的输出则是：(batch, 25, 13, 13)\n",
    "        # 第三层：数据输入是：(batch, 25, 13, 13)，经过卷积，第三层的输出则是：(batch, 50, 11, 11)\n",
    "        self.layer3 = nn.Sequential(\n",
    "                      nn.Conv2d(25, 50, 3),\n",
    "                      nn.BatchNorm2d(50),\n",
    "                      nn.ReLU())\n",
    "        # 第四层是池化层，池化窗口大小为(2, 2)，数据输入是：(batch, 50, 11, 11)，经过池化，第四层的输出则是：(batch, 50, 5, 5)\n",
    "        # 最后是线性层，共两层：\n",
    "        self.fc = nn.Sequential(\n",
    "                      nn.Linear(50 * 5 * 5, 1024),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(1024, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 10))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.pool(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.pool(out)\n",
    "        out = out.view(out.shape[0], -1)  # 将图片拉伸（展平），变成向量，向量的大小为 50 X 5 X 5，out.shape[0]等于batch\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, train loss: 0.1731, train acc: 0.9470, test loss: 0.0466, test acc: 0.9854\n",
      "epoch:  2, train loss: 0.1134, train acc: 0.9694, test loss: 0.0338, test acc: 0.9894\n",
      "epoch:  3, train loss: 0.0364, train acc: 0.9887, test loss: 0.0270, test acc: 0.9920\n",
      "epoch:  4, train loss: 0.0246, train acc: 0.9923, test loss: 0.0346, test acc: 0.9894\n",
      "epoch:  5, train loss: 0.0179, train acc: 0.9947, test loss: 0.0297, test acc: 0.9914\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "Epochs = 5\n",
    "for epoch in range(Epochs):\n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    for image, label in train_data:\n",
    "        image = Variable(image)\n",
    "        label = Variable(label)\n",
    "        # 前向传播\n",
    "        y_pred = net(image)\n",
    "        loss = criterion(y_pred, label)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.data.item()\n",
    "        _, out = y_pred.max(1)                     # 返回每一行最大值对应的下标，就是图片的预测值\n",
    "        num_correct = (out == label).sum().item()  # 统计预测正确的数量\n",
    "        acc = num_correct / image.shape[0]         # 得到这一个batch的平均准确率\n",
    "        acc_sum += acc\n",
    "    ave_train_loss = loss_sum / len(train_data)\n",
    "    ave_train_acc = acc_sum / len(train_data)\n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    # 在测试集上检验效果\n",
    "    net.eval()  # 将模型改为预测模式，eval（）时，pytorch会自动把BN和DropOut固定住，不会取平均，而是用训练好的值。\n",
    "    for image, label in test_data:\n",
    "        image = Variable(image)\n",
    "        label = Variable(label)\n",
    "        y_pred = net(image)\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss_sum += loss.data.item()\n",
    "        _, out = y_pred.max(1)\n",
    "        num_correct = (out == label).sum().item()\n",
    "        acc = num_correct / image.shape[0]\n",
    "        acc_sum += acc\n",
    "    ave_test_loss = loss_sum / len(test_data)\n",
    "    ave_test_acc = acc_sum / len(test_data)\n",
    "    print('epoch: {:2d}, train loss: {:.4f}, train acc: {:.4f}, test loss: {:.4f}, test acc: {:.4f}'.format(epoch + 1, ave_train_loss, ave_train_acc, ave_test_loss, ave_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
